{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353c5565-cdb5-4111-a4bc-2b3a7b007202",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932aaad0-46de-4e22-a354-0835c3b21c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from names_dataset import NameDataset\n",
    "\n",
    "nd = NameDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b45c5bdf-4239-46c0-a5d3-19e2f6f42de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(nd.search(\"Angel\")[\"first_name\"][\"country\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ff74f2-5931-4bc1-a6fc-e78749942ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_es = pd.read_csv(\"./name_dataset/ES.csv\", names = [\"first_name\", \"last_name\", \"gender\", \"alpha2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a13b22d-e9dd-40a6-8f6a-2056305b82e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10891211"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e7bc574-f61e-4cd0-a112-bb44ad3a09f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10858177"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es.dropna(subset = [\"first_name\", \"last_name\"], inplace=True)\n",
    "len(df_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed015ac8-3ead-42cb-85b4-5a4200db9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "df_es[\"first_name_unidecoded\"] = df_es.first_name.apply(unidecode)\n",
    "df_es[\"last_name_unidecoded\"] = df_es.last_name.apply(unidecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d9f0ed7-762f-4b65-8258-c1363e8ad45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 0: Hello! How can I assist you with your request today? (Time taken: 1.52 sec)\n",
      "Request 1: Hello! How can I assist you with your request? (Time taken: 1.13 sec)\n",
      "Request 2: Hello! How can I assist you today with your request? (Time taken: 1.15 sec)\n",
      "Request 3: Hello! How can I assist you today with your request? (Time taken: 1.30 sec)\n",
      "\n",
      "Total elapsed time: 1.52 sec\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import time\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Patch asyncio for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the async OpenAI client\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "async def get_completion(i):\n",
    "    start_time = time.time()\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Hello, this is request {i}.\"}]\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return f\"Request {i}: {response.choices[0].message.content} (Time taken: {elapsed_time:.2f} sec)\"\n",
    "\n",
    "async def main():\n",
    "    start = time.time()\n",
    "    tasks = [get_completion(i) for i in range(4)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    for result in results:\n",
    "        print(result)\n",
    "    \n",
    "    print(f\"\\nTotal elapsed time: {total_time:.2f} sec\")\n",
    "\n",
    "# Run the async function in Jupyter\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54a3bde4-1d82-421e-99c1-b08a26899b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be79fcf1-455f-4a5f-9a3d-cabd8f15a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia (0.43 sec)\n",
      "Antonia (0.45 sec)\n",
      "María (0.48 sec)\n",
      "Inés (0.42 sec)\n",
      "Miguel Ángel (0.48 sec)\n",
      "Héctor (0.44 sec)\n",
      "Mónica (0.48 sec)\n",
      "Begoña (0.45 sec)\n",
      "Silvia (0.49 sec)\n",
      "Álex (0.50 sec)\n",
      "Lidia (0.47 sec)\n",
      "Belén (0.48 sec)\n",
      "Iván (0.52 sec)\n",
      "Ángela (0.49 sec)\n",
      "Raúl (0.52 sec)\n",
      "Víctor (0.52 sec)\n",
      "José Manuel (0.52 sec)\n",
      "Rocío (0.52 sec)\n",
      "Lucía (0.51 sec)\n",
      "Gloria (0.49 sec)\n",
      "César (0.49 sec)\n",
      "Ana María (0.51 sec)\n",
      "José (0.56 sec)\n",
      "Ramón (0.53 sec)\n",
      "Joaquín (0.53 sec)\n",
      "Juan José (0.53 sec)\n",
      "Rosa María (0.52 sec)\n",
      "Miriam (0.53 sec)\n",
      "Sonia (0.56 sec)\n",
      "Núria (0.57 sec)\n",
      "Félix (0.54 sec)\n",
      "Julián (0.55 sec)\n",
      "Toñi (0.58 sec)\n",
      "Óscar (0.61 sec)\n",
      "Claudia (0.59 sec)\n",
      "Verónica (0.63 sec)\n",
      "Adrián (0.66 sec)\n",
      "María Jesús (0.64 sec)\n",
      "Tomás (0.64 sec)\n",
      "José Luis (0.70 sec)\n",
      "Jesús (0.74 sec)\n",
      "Ángel (0.74 sec)\n",
      "Ángeles (0.71 sec)\n",
      "Andrés (0.75 sec)\n",
      "Agustín (0.74 sec)\n",
      "Natalia (0.78 sec)\n",
      "José María (0.78 sec)\n",
      "Fátima (0.78 sec)\n",
      "María José (0.91 sec)\n",
      "Álvaro (0.96 sec)\n",
      "José Antonio (1.04 sec)\n",
      "Rubén (6.11 sec)\n",
      "\n",
      "Total elapsed time: 6.13 sec\n",
      "Fernández (0.87 sec)\n",
      "Martín (0.86 sec)\n",
      "Gómez (0.86 sec)\n",
      "Méndez (0.85 sec)\n",
      "Sánchez (0.88 sec)\n",
      "Giménez (0.85 sec)\n",
      "Cortés (0.86 sec)\n",
      "López (0.89 sec)\n",
      "Marín (0.87 sec)\n",
      "López López (0.85 sec)\n",
      "Muñoz (0.88 sec)\n",
      "Jiménez (0.89 sec)\n",
      "González García (0.84 sec)\n",
      "Ibáñez (0.86 sec)\n",
      "Díaz (0.90 sec)\n",
      "Román (0.88 sec)\n",
      "Núñez (0.90 sec)\n",
      "García García (0.92 sec)\n",
      "Domínguez (0.93 sec)\n",
      "García Rodríguez (0.90 sec)\n",
      "Martínez (0.96 sec)\n",
      "Ramírez (0.95 sec)\n",
      "Caño (0.93 sec)\n",
      "García Fernández (0.92 sec)\n",
      "Cámara (0.86 sec)\n",
      "Peña (0.95 sec)\n",
      "Rodríguez (0.98 sec)\n",
      "Pérez García (0.91 sec)\n",
      "García Sánchez (0.94 sec)\n",
      "Millán (0.82 sec)\n",
      "García Martínez (0.94 sec)\n",
      "Antón (0.79 sec)\n",
      "Fernández González (0.90 sec)\n",
      "Benítez (0.96 sec)\n",
      "González López (0.85 sec)\n",
      "Martín González (0.76 sec)\n",
      "Álvarez (0.99 sec)\n",
      "Del Río (0.86 sec)\n",
      "González (1.02 sec)\n",
      "Más (0.86 sec)\n",
      "Zaragoza (0.69 sec)\n",
      "Fernández Sánchez (0.84 sec)\n",
      "García Díaz (0.80 sec)\n",
      "Martínez González (0.83 sec)\n",
      "González Martínez (0.84 sec)\n",
      "Sánchez García (0.98 sec)\n",
      "Pinto (0.82 sec)\n",
      "Valdés (0.71 sec)\n",
      "Alcalá (0.67 sec)\n",
      "Ruiz Martínez (0.67 sec)\n",
      "Martínez Fernández (0.88 sec)\n",
      "Gómez González (0.76 sec)\n",
      "Díaz García (0.82 sec)\n",
      "Pérez Martínez (0.87 sec)\n",
      "Ruiz Sánchez (0.73 sec)\n",
      "Martínez Martínez (0.99 sec)\n",
      "Pérez Sánchez (0.87 sec)\n",
      "Martínez Pérez (0.88 sec)\n",
      "López González (0.90 sec)\n",
      "Mejía (0.85 sec)\n",
      "Traoré (0.84 sec)\n",
      "García González (1.02 sec)\n",
      "Guillén (0.98 sec)\n",
      "Durán (1.03 sec)\n",
      "Rincón (0.79 sec)\n",
      "Piña (0.75 sec)\n",
      "Catalán (0.71 sec)\n",
      "San (0.82 sec)\n",
      "Márquez (1.05 sec)\n",
      "Muñoz García (0.78 sec)\n",
      "Pérez (1.08 sec)\n",
      "Rodríguez López (0.96 sec)\n",
      "Chacón (0.86 sec)\n",
      "Pérez Rodríguez (0.96 sec)\n",
      "Marco (0.88 sec)\n",
      "Juárez (0.84 sec)\n",
      "López Jiménez (0.75 sec)\n",
      "García Ruiz (0.89 sec)\n",
      "Fernández Fernández (1.05 sec)\n",
      "Martín Pérez (0.79 sec)\n",
      "Casas (1.00 sec)\n",
      "Suárez (1.08 sec)\n",
      "Vásquez (0.90 sec)\n",
      "Nicolás (0.83 sec)\n",
      "Vila (1.02 sec)\n",
      "Díaz González (0.80 sec)\n",
      "Gutiérrez (1.10 sec)\n",
      "Aragón (0.88 sec)\n",
      "Gómez Fernández (0.82 sec)\n",
      "Martínez Sánchez (0.97 sec)\n",
      "Pérez Pérez (1.05 sec)\n",
      "Martín Rodríguez (0.88 sec)\n",
      "Rodríguez Díaz (0.76 sec)\n",
      "Araújo (0.82 sec)\n",
      "Cárdenas (0.93 sec)\n",
      "Solá (0.85 sec)\n",
      "Dávila (0.80 sec)\n",
      "Fernández Martín (0.77 sec)\n",
      "Ruiz García (0.90 sec)\n",
      "Avilés (0.76 sec)\n",
      "Fernández Martínez (0.95 sec)\n",
      "Díaz Rodríguez (0.75 sec)\n",
      "Rodríguez Hernández (0.81 sec)\n",
      "Pérez Martín (0.86 sec)\n",
      "Jiménez Jiménez (0.93 sec)\n",
      "Menéndez (0.98 sec)\n",
      "Gómez García (0.98 sec)\n",
      "García (1.15 sec)\n",
      "Páez (0.79 sec)\n",
      "Roldán (1.05 sec)\n",
      "Sánchez Rodríguez (0.99 sec)\n",
      "González Fernández (1.05 sec)\n",
      "Ramón (0.95 sec)\n",
      "España (0.95 sec)\n",
      "Gómez López (0.89 sec)\n",
      "Gómez Gómez (0.93 sec)\n",
      "Martín López (0.85 sec)\n",
      "Pérez López (1.01 sec)\n",
      "López García (1.11 sec)\n",
      "Rodríguez García (1.09 sec)\n",
      "Bermúdez (1.05 sec)\n",
      "Martínez Gómez (0.84 sec)\n",
      "Gómez Pérez (0.86 sec)\n",
      "López Pérez (1.02 sec)\n",
      "Vázquez (1.14 sec)\n",
      "Domènech (0.91 sec)\n",
      "Díaz Díaz (0.81 sec)\n",
      "Ocaña (0.83 sec)\n",
      "Sánchez Pérez (0.97 sec)\n",
      "Fernández Díaz (0.79 sec)\n",
      "López Rodríguez (1.04 sec)\n",
      "Morán (0.95 sec)\n",
      "López Ruiz (0.87 sec)\n",
      "León (1.14 sec)\n",
      "Carrión (0.92 sec)\n",
      "González Hernández (0.86 sec)\n",
      "Tomás (1.00 sec)\n",
      "Gálvez (1.05 sec)\n",
      "Santamaría (1.04 sec)\n",
      "Sánchez González (1.00 sec)\n",
      "Hernández González (0.85 sec)\n",
      "Hernández Rodríguez (0.84 sec)\n",
      "Sáez (1.10 sec)\n",
      "Solís (0.93 sec)\n",
      "González Gómez (0.91 sec)\n",
      "María (1.15 sec)\n",
      "Guzmán (1.11 sec)\n",
      "González Pérez (1.06 sec)\n",
      "García Muñoz (0.92 sec)\n",
      "Galán (1.11 sec)\n",
      "Martínez Ruiz (0.84 sec)\n",
      "Sánchez Sánchez (1.12 sec)\n",
      "Fernández Rodríguez (1.08 sec)\n",
      "José (1.12 sec)\n",
      "Rodríguez (0.93 sec)\n",
      "Fernández Gómez (0.91 sec)\n",
      "Jiménez López (0.83 sec)\n",
      "Sánchez Jiménez (0.84 sec)\n",
      "García López (1.16 sec)\n",
      "Sánchez Gómez (0.94 sec)\n",
      "Velásquez (0.83 sec)\n",
      "Díaz Fernández (0.82 sec)\n",
      "Alcántara (0.93 sec)\n",
      "Piñeiro (0.86 sec)\n",
      "Gómez Rodríguez (0.93 sec)\n",
      "Gámez (0.89 sec)\n",
      "Vélez (0.97 sec)\n",
      "Córdoba (1.02 sec)\n",
      "Lázaro (1.02 sec)\n",
      "Martínez López (1.10 sec)\n",
      "Sánchez Fernández (1.03 sec)\n",
      "Ló (0.83 sec)\n",
      "Martín Fernández (0.88 sec)\n",
      "Jiménez García (1.00 sec)\n",
      "Ivanova (0.85 sec)\n",
      "Belló (0.89 sec)\n",
      "Peláez (0.93 sec)\n",
      "Estévez (1.10 sec)\n",
      "Ríos (1.14 sec)\n",
      "Ruiz López (0.94 sec)\n",
      "Sanchís (0.91 sec)\n",
      "García Jiménez (1.01 sec)\n",
      "Marqués (0.97 sec)\n",
      "García Moreno (1.01 sec)\n",
      "Martínez Rodríguez (1.00 sec)\n",
      "Macías (1.15 sec)\n",
      "Martínez García (1.17 sec)\n",
      "López Sánchez (1.11 sec)\n",
      "Fernández Álvarez (0.90 sec)\n",
      "Castaño (1.07 sec)\n",
      "García Hernández (1.01 sec)\n",
      "Beltrán (1.13 sec)\n",
      "Simón (1.09 sec)\n",
      "Rodríguez Gómez (0.95 sec)\n",
      "Ángulo (0.92 sec)\n",
      "Martín Sánchez (1.00 sec)\n",
      "Pérez Fernández (1.06 sec)\n",
      "Rodríguez González (1.15 sec)\n",
      "López Díaz (0.87 sec)\n",
      "Rodríguez Martín (1.00 sec)\n",
      "Álvarez García (0.96 sec)\n",
      "Andrés (1.16 sec)\n",
      "González Martín (1.02 sec)\n",
      "Arévalo (0.98 sec)\n",
      "Jiménez Sánchez (0.91 sec)\n",
      "Martí (1.18 sec)\n",
      "Álvarez Fernández (0.94 sec)\n",
      "Jesús (0.97 sec)\n",
      "Rodríguez Pérez (1.13 sec)\n",
      "López Martín (0.96 sec)\n",
      "Rodríguez Rodríguez (1.19 sec)\n",
      "González Álvarez (0.90 sec)\n",
      "Muñiz (0.92 sec)\n",
      "Luis (1.14 sec)\n",
      "Ordóñez (1.11 sec)\n",
      "García Pérez (1.20 sec)\n",
      "Blázquez (1.09 sec)\n",
      "Pérez Hernández (0.92 sec)\n",
      "Expósito (1.18 sec)\n",
      "Pérez González (1.15 sec)\n",
      "Fernández García (1.23 sec)\n",
      "Martín Martín (1.15 sec)\n",
      "González González (1.21 sec)\n",
      "Cáceres (1.13 sec)\n",
      "Mejías (0.97 sec)\n",
      "González Rodríguez (1.21 sec)\n",
      "González Sánchez (1.12 sec)\n",
      "Fernández Pérez (1.13 sec)\n",
      "López Martínez (1.20 sec)\n",
      "Velázquez (1.10 sec)\n",
      "López Gómez (1.06 sec)\n",
      "García Martín (1.17 sec)\n",
      "Linares (1.12 sec)\n",
      "Sánchez Martín (1.08 sec)\n",
      "Sáiz (1.05 sec)\n",
      "García Gómez (1.17 sec)\n",
      "Sánchez López (1.21 sec)\n",
      "Hernández (1.31 sec)\n",
      "Gómez Sánchez (1.07 sec)\n",
      "Moreno García (1.06 sec)\n",
      "Calderón (1.24 sec)\n",
      "Romero García (0.95 sec)\n",
      "Ángel (1.19 sec)\n",
      "García Álvarez (1.07 sec)\n",
      "Marí (1.08 sec)\n",
      "Baldé (1.07 sec)\n",
      "Hernández García (1.11 sec)\n",
      "Chávez (1.09 sec)\n",
      "Díez (1.30 sec)\n",
      "Hernández Pérez (0.99 sec)\n",
      "Rodríguez Álvarez (1.01 sec)\n",
      "López Fernández (1.25 sec)\n",
      "Álvarez González (0.99 sec)\n",
      "Hernández Hernández (1.07 sec)\n",
      "Gómez Martínez (1.05 sec)\n",
      "Fernández López (1.27 sec)\n",
      "González Díaz (1.08 sec)\n",
      "Rodríguez Martínez (1.21 sec)\n",
      "Sánchez Martínez (1.28 sec)\n",
      "Rodríguez Fernández (1.33 sec)\n",
      "Gracia (1.32 sec)\n",
      "Galván (1.11 sec)\n",
      "González (1.35 sec)\n",
      "Pérez Gómez (1.20 sec)\n",
      "Sánchez Ruiz (1.22 sec)\n",
      "Martín García (1.55 sec)\n",
      "Báez (1.50 sec)\n",
      "Falcón (1.88 sec)\n",
      "Ávila (2.12 sec)\n",
      "Alarcón (2.19 sec)\n",
      "Rodríguez Sánchez (3.06 sec)\n",
      "\n",
      "Total elapsed time: 3.24 sec\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "async def fetch_canonical_name(prompt: str) -> str:\n",
    "    \"\"\"Calls GPT to return the canonical version from a list of name variations.\"\"\"\n",
    "    start_time = time.time()\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert name corrector. Respond only with the right name from the options provided\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=10,\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    canonical_name = response.choices[0].message.content.strip()\n",
    "    print(f\"{canonical_name} ({elapsed_time:.2f} sec)\")\n",
    "    return canonical_name\n",
    "\n",
    "def get_name_prompts(df, name_column, unidecoded_column, threshold_pctg = 0.001, group_threshold_pctg = 0.01):\n",
    "    \"\"\"\n",
    "    Extracts names and prepares prompts.\n",
    "    \n",
    "    For each unidecoded name that appears more than a threshold (1% of total rows),\n",
    "    it collects variations from the specified name_column using a sub-threshold (1% of group count).\n",
    "    Only if there are multiple relevant variations, it returns a prompt.\n",
    "    \n",
    "    Returns a list of tuples: (unidecoded name, prompt)\n",
    "    \"\"\"\n",
    "    threshold = threshold_pctg * len(df)\n",
    "    counts = df[unidecoded_column].value_counts()\n",
    "    selected_names = counts[counts > threshold].index\n",
    "\n",
    "    prompts = []\n",
    "    # Group once to avoid repeated heavy extraction\n",
    "    groups = df.groupby(unidecoded_column)[name_column]\n",
    "    for uname in selected_names:\n",
    "        group = groups.get_group(uname)\n",
    "        var_counts = group.value_counts()\n",
    "        sub_threshold = group_threshold_pctg * group.count()\n",
    "        variations = var_counts[var_counts >= sub_threshold].index.tolist()\n",
    "        if len(variations) > 1:\n",
    "            prompt = f\"Given the following variations: {', '.join(variations)}, what is the correct accepted version in Spanish?\"\n",
    "            prompts.append((uname, prompt))\n",
    "    return prompts\n",
    "\n",
    "async def run_api_calls(prompts):\n",
    "    \"\"\"\n",
    "    Runs GPT calls concurrently for each prompt.\n",
    "    \n",
    "    Expects a list of tuples (unidecoded name, prompt) and returns a mapping:\n",
    "    {unidecoded name: canonical name}\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    tasks = [fetch_canonical_name(prompt) for _, prompt in prompts]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    total_time = time.time() - start\n",
    "    print(f\"\\nTotal elapsed time: {total_time:.2f} sec\")\n",
    "    return {uname: canonical for (uname, _), canonical in zip(prompts, responses)}\n",
    "\n",
    "# Process first names\n",
    "prompts_first = get_name_prompts(df_es, \"first_name\", \"first_name_unidecoded\")\n",
    "name_map_first = await run_api_calls(prompts_first)\n",
    "df_es['first_name_corrected'] = df_es['first_name_unidecoded'].map(name_map_first).fillna(df_es['first_name'])\n",
    "\n",
    "# Process surnames similarly\n",
    "prompts_surname = get_name_prompts(df_es, \"last_name\", \"last_name_unidecoded\", 0.0001, 0.01)\n",
    "name_map_surname = await run_api_calls(prompts_surname)\n",
    "df_es['last_name_corrected'] = df_es['last_name_unidecoded'].map(name_map_surname).fillna(df_es['last_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cddf8dcb-6038-4035-adfb-c2a1ab9e5335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2171341)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df_es.first_name != df_es.first_name_corrected )| (df_es.last_name != df_es.last_name_corrected)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc486dc7-fa0c-4871-a71d-2f13f59b86fd",
   "metadata": {},
   "source": [
    "### Cleaning names which are not common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "90cf20df-55a0-43d5-9818-69e2900b4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df_es.first_name_corrected.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "63636c41-cbd0-4dcf-887e-2c98d7cadc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Threshold  Percentage Left\n",
      "0           1       100.000000\n",
      "1           2        95.599749\n",
      "2           3        94.527820\n",
      "3           4        93.910783\n",
      "4           5        93.457926\n",
      "5           6        93.090415\n",
      "6           7        92.772461\n",
      "7           8        92.500150\n",
      "8           9        92.257973\n",
      "9          10        92.036582\n",
      "10         11        91.838391\n",
      "11         12        91.657964\n",
      "12         13        91.484565\n",
      "13         14        91.328084\n",
      "14         15        91.184966\n",
      "15         16        91.043229\n",
      "16         17        90.903832\n",
      "17         18        90.772475\n",
      "18         19        90.654113\n"
     ]
    }
   ],
   "source": [
    "total_count = names.sum()\n",
    "\n",
    "# Calculate the percentage of data left for each threshold from 1 to 10\n",
    "percentages = {}\n",
    "for i in range(1, 20):\n",
    "    filtered_sum = names[names >= i].sum()\n",
    "    percentages[i] = (filtered_sum / total_count) * 100\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "percentages_df = pd.DataFrame(list(percentages.items()), columns=[\"Threshold\", \"Percentage Left\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(percentages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cde71759-6202-4833-85ff-3300d6f7689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['José', 'María', 'Antonio', 'Ana', 'Juan', 'Carmen', 'Manuel', 'David',\n",
      "       'Carlos', 'Javier',\n",
      "       ...\n",
      "       'Holga', 'Ramone', 'Horus', 'Faru', 'Elena Beatriz', 'Wale',\n",
      "       'Juana Del Carmen', 'Luisa Pilar', 'Adriam', 'Pepe Toni'],\n",
      "      dtype='object', name='first_name_corrected', length=27266)\n"
     ]
    }
   ],
   "source": [
    "# We filter only by names with more than 11 appearances\n",
    "valid_names = names[names >= 11].index\n",
    "print(valid_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9a66055f-510b-49db-b114-a43662c71f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_es = df_es[df_es.first_name_corrected.isin(valid_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d583cf6b-9844-4d6b-9645-ba37ecb8c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter out rows in which surname and name are exactly the same\n",
    "df_es = df_es[df_es.first_name != df_es.last_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b951b7ed-45e5-4f28-8577-03c1aaa5fb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['first_name', 'last_name', 'gender', 'alpha2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# We get the DataFrame with the corrected names and the other desired columns\n",
    "df_es = df_es[['first_name_corrected', 'last_name_corrected', 'gender', 'alpha2']].copy()\n",
    "\n",
    "# Rename the corrected columns to the standard names\n",
    "df_es.rename(columns={\n",
    "    'first_name_corrected': 'first_name',\n",
    "    'last_name_corrected': 'last_name'\n",
    "}, inplace=True)\n",
    "\n",
    "# Optionally, inspect the new DataFrame\n",
    "print(df_es.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d2003-1251-45bf-bf1a-c16d45bcf601",
   "metadata": {},
   "source": [
    "## Gender assignation\n",
    "We assign gender to the names without by computing the most common gender for each name and filling the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "486e9f4f-031a-400e-b490-005a8e051a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>gender</th>\n",
       "      <th>first_name</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>dominant_ratio</th>\n",
       "      <th>dominant_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10493</th>\n",
       "      <td>Iratxe</td>\n",
       "      <td>1228</td>\n",
       "      <td>1</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7446</th>\n",
       "      <td>Estibaliz</td>\n",
       "      <td>1227</td>\n",
       "      <td>1</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Adrià</td>\n",
       "      <td>2</td>\n",
       "      <td>1689</td>\n",
       "      <td>844.5</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15418</th>\n",
       "      <td>Maialen</td>\n",
       "      <td>843</td>\n",
       "      <td>1</td>\n",
       "      <td>843.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11884</th>\n",
       "      <td>Jorge Luis</td>\n",
       "      <td>0</td>\n",
       "      <td>811</td>\n",
       "      <td>811.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>Blue</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>Blair</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23587</th>\n",
       "      <td>Snoopy</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25078</th>\n",
       "      <td>Valentine</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20835</th>\n",
       "      <td>Pip</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27266 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "gender  first_name     F     M  dominant_ratio dominant_gender\n",
       "10493       Iratxe  1228     1          1228.0               F\n",
       "7446     Estibaliz  1227     1          1227.0               F\n",
       "433          Adrià     2  1689           844.5               M\n",
       "15418      Maialen   843     1           843.0               F\n",
       "11884   Jorge Luis     0   811           811.0               M\n",
       "...            ...   ...   ...             ...             ...\n",
       "3485          Blue     3     3             1.0               M\n",
       "3434         Blair     5     5             1.0               M\n",
       "23587       Snoopy     6     6             1.0               M\n",
       "25078    Valentine    14    14             1.0               M\n",
       "20835          Pip     7     7             1.0               M\n",
       "\n",
       "[27266 rows x 5 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count occurrences of each (first_name, gender) pair\n",
    "df_counts = df_es.groupby([\"first_name\", \"gender\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Ensure the columns are treated as numeric\n",
    "df_counts = df_counts.astype(int)\n",
    "\n",
    "\n",
    "# Compute the dominant ratio (larger count divided by the smaller count)\n",
    "df_counts[\"dominant_ratio\"] = df_counts.max(axis=1) / df_counts.min(axis=1).replace(0, 1)  # Avoid division by zero\n",
    "\n",
    "# Compute the dominant gender\n",
    "df_counts[\"dominant_gender\"] = df_counts[[\"M\", \"F\"]].idxmax(axis=1)\n",
    "\n",
    "# Reset index for a cleaner DataFrame\n",
    "df_counts = df_counts.reset_index()\n",
    "\n",
    "# Sort by ratio in descending order\n",
    "df_counts = df_counts.sort_values(by=\"dominant_ratio\", ascending=False)\n",
    "\n",
    "# Display the result\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7bcc8a4a-b550-415c-937b-356d07e58662",
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_gender_names = df_counts[df_counts.dominant_ratio > 5].set_index(\"first_name\").dominant_gender.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e4dfa352-f87e-4a27-a738-2af997ded95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_es[\"gender\"] = df_es[\"gender\"].fillna(df_es[\"first_name\"].map(dominant_gender_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e4338b73-dd45-4caa-88db-2f3f62a8aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some rows without gender but we can drop them safely\n",
    "df_es.gender.isna().sum()\n",
    "\n",
    "df_es.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7b7882aa-da29-4a4a-8e55-c3c331ca7ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9089856031620359"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have now cleaned the dataset. We remain with 90% of the data.\n",
    "df_es.shape[0]/10891211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1cc5c919-15a7-4813-bf79-a57b6a35de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_es.to_parquet(\"./df_es_clean.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898398e4-37e7-4112-98c0-340b1c8339aa",
   "metadata": {},
   "source": [
    "## Creating Training dataset\n",
    "\n",
    "The goal here is to build a model which can \n",
    "- reconstruct names from misspelled forms (oscar -> Óscar)\n",
    "- separate first_name from last_name (oscar sanchez -> Óscar, Sánchez\n",
    "- estimate the gender given name (oscar sanchez) -> M\n",
    "\n",
    "The input of the model is a string, the output is a JSON of the form\n",
    "\n",
    "```\n",
    "input: \"oscar sanchez\"\n",
    "output:\n",
    "{\n",
    "    \"first_name\" : \"Óscar\",\n",
    "    \"last_name\" : \"Sánchez\",\n",
    "    \"gender\" : \"M\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "86f9f5ac-33bc-4555-b265-06f7f827996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        first_name        last_name gender alpha2\n",
      "2267206   Gabriela         Mircheva      F     ES\n",
      "5196677      María            Otero      F     ES\n",
      "8069579   Fernando  Rodríguez Tapia      M     ES\n",
      "5027431     Carlos          Roberto      M     ES\n"
     ]
    }
   ],
   "source": [
    "print(df_es.sample(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3bcc3f7-8819-4a0a-903e-119dd710cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: correct\n",
      "Input: Alberto Cea\n",
      "Output: {\"first_name\": \"Alberto\", \"last_name\": \"Cea\", \"gender\": \"M\"}\n",
      "--------------------------------------------------\n",
      "Transformation: remove_accents_and_lower\n",
      "Input: alberto cea\n",
      "Output: {\"first_name\": \"Alberto\", \"last_name\": \"Cea\", \"gender\": \"M\"}\n",
      "--------------------------------------------------\n",
      "Transformation: insert_extra_spaces\n",
      "Input: Alberto   Cea\n",
      "Output: {\"first_name\": \"Alberto\", \"last_name\": \"Cea\", \"gender\": \"M\"}\n",
      "--------------------------------------------------\n",
      "Transformation: last_name_only\n",
      "Input: Cea\n",
      "Output: {\"first_name\": null, \"last_name\": \"Cea\", \"gender\": \"M\"}\n",
      "--------------------------------------------------\n",
      "Transformation: correct\n",
      "Input: Adrián Gómez\n",
      "Output: {\"first_name\": \"Adrián\", \"last_name\": \"Gómez\", \"gender\": \"M\"}\n",
      "--------------------------------------------------\n",
      "Transformation: random_uppercase\n",
      "Input: AdriáN GÓmez\n",
      "Output: {\"first_name\": \"Adrián\", \"last_name\": \"Gómez\", \"gender\": \"M\"}\n",
      "--------------------------------------------------\n",
      "Transformation: duplicate_pair\n",
      "Input: Adrián Gómez Adrián Gómez\n",
      "Output: {\"first_name\": \"Adrián\", \"last_name\": \"Gómez\", \"gender\": \"M\"}\n",
      "--------------------------------------------------\n",
      "Transformation: first_name_only\n",
      "Input: Adrián\n",
      "Output: {\"first_name\": \"Adrián\", \"last_name\": null, \"gender\": \"M\"}\n",
      "--------------------------------------------------\n",
      "Transformation: correct\n",
      "Input: Ángela Buraga\n",
      "Output: {\"first_name\": \"Ángela\", \"last_name\": \"Buraga\", \"gender\": \"F\"}\n",
      "--------------------------------------------------\n",
      "Transformation: remove_accents_and_lower\n",
      "Input: angela buraga\n",
      "Output: {\"first_name\": \"Ángela\", \"last_name\": \"Buraga\", \"gender\": \"F\"}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Transformation functions.\n",
    "def transform_correct(text):\n",
    "    return text\n",
    "\n",
    "def transform_remove_accents_and_lower(text):\n",
    "    return unidecode(text).lower()\n",
    "\n",
    "def transform_insert_extra_spaces(text, extra_space_range=(1, 3)):\n",
    "    words = text.split()\n",
    "    return \" \".join(word + \" \" * random.randint(*extra_space_range) for word in words).strip()\n",
    "\n",
    "def transform_random_uppercase(text, uppercase_prob=0.1):\n",
    "    noisy_chars = []\n",
    "    for char in text:\n",
    "        if char.isalpha() and random.random() < uppercase_prob:\n",
    "            noisy_chars.append(char.upper())\n",
    "        else:\n",
    "            noisy_chars.append(char)\n",
    "    return \"\".join(noisy_chars)\n",
    "\n",
    "def transform_duplicate_pair(text):\n",
    "    # Duplicate the full name, e.g. \"Jaime Pérez\" -> \"Jaime Pérez Jaime Pérez\"\n",
    "    return f\"{text} {text}\"\n",
    "\n",
    "def transform_no_space(text):\n",
    "    # Remove the space between first and last names.\n",
    "    return \"\".join(text.split())\n",
    "\n",
    "def transform_first_name_only(sample):\n",
    "    # Return only the first name.\n",
    "    return sample['first_name']\n",
    "    \n",
    "def transform_last_name_only(sample):\n",
    "    # Return only the last name.\n",
    "    return sample['last_name']\n",
    "\n",
    "def transform_duplicates_end_character(sample):\n",
    "    # maria -> mariaa ; Juan Luis -> Juan Luiss\n",
    "    return sample['first_name'] + sample['first_name'][-1]\n",
    "    \n",
    "\n",
    "def generate_noisy_inputs(sample, transform_probs):\n",
    "    full_name = f\"{sample['first_name']} {sample['last_name']}\"\n",
    "    outputs = []\n",
    "    # Each effect is applied independently.\n",
    "    DEFAULT_PROB = 0.01\n",
    "    if random.random() < transform_probs.get('correct', DEFAULT_PROB):\n",
    "        outputs.append(('correct', transform_correct(full_name)))\n",
    "    if random.random() < transform_probs.get('remove_accents_and_lower', DEFAULT_PROB):\n",
    "        outputs.append(('remove_accents_and_lower', transform_remove_accents_and_lower(full_name)))\n",
    "    if random.random() < transform_probs.get('insert_extra_spaces', DEFAULT_PROB):\n",
    "        outputs.append(('insert_extra_spaces', transform_insert_extra_spaces(full_name)))\n",
    "    if random.random() < transform_probs.get('random_uppercase', DEFAULT_PROB):\n",
    "        outputs.append(('random_uppercase', transform_random_uppercase(full_name)))\n",
    "    if random.random() < transform_probs.get('duplicate_pair', DEFAULT_PROB):\n",
    "        outputs.append(('duplicate_pair', transform_duplicate_pair(full_name)))\n",
    "    if random.random() < transform_probs.get('no_space', DEFAULT_PROB):\n",
    "        outputs.append(('no_space', transform_no_space(full_name)))\n",
    "    if random.random() < transform_probs.get('first_name_only', DEFAULT_PROB):\n",
    "        outputs.append(('first_name_only', transform_first_name_only(sample)))\n",
    "    if random.random() < transform_probs.get('last_name_only', DEFAULT_PROB):\n",
    "        outputs.append(('last_name_only', transform_last_name_only(sample)))\n",
    "    if random.random() < transform_probs.get('transform_duplicates_end_character',DEFAULT_PROB):\n",
    "        outputs.append(('transform_duplicates_end_character', transform_last_name_only(sample)))    \n",
    "    return outputs\n",
    "\n",
    "def create_training_examples(row, transform_probs):\n",
    "    noisy_versions = generate_noisy_inputs(row, transform_probs)\n",
    "    examples = []\n",
    "    for trans_name, noisy_input in noisy_versions:\n",
    "        # Adjust target based on the transformation.\n",
    "        if trans_name == 'first_name_only':\n",
    "            target = {\"first_name\": row[\"first_name\"], \"last_name\": None, \"gender\": row[\"gender\"]}\n",
    "        elif trans_name == 'last_name_only':\n",
    "            target = {\"first_name\": None, \"last_name\": row[\"last_name\"], \"gender\": row[\"gender\"]}\n",
    "        else:\n",
    "            target = {\"first_name\": row[\"first_name\"], \"last_name\": row[\"last_name\"], \"gender\": row[\"gender\"]}\n",
    "        target_json = json.dumps(target, ensure_ascii=False)\n",
    "        examples.append({\"input\": noisy_input, \"output\": target_json, \"transformation\": trans_name})\n",
    "    return examples\n",
    "\n",
    "\n",
    "df_es = pd.read_parquet(\"./df_es_clean.parquet\")\n",
    "\n",
    "# Define transformation probabilities.\n",
    "transform_probs = {\n",
    "    \"correct\": 1.0,                   \n",
    "    \"remove_accents_and_lower\": 0.7,    \n",
    "    \"insert_extra_spaces\": 0.3,         \n",
    "    \"random_uppercase\": 0.1,            \n",
    "    \"duplicate_pair\": 0.1,              \n",
    "    \"no_space\": 0.3,\n",
    "    \"first_name_only\": 0.2,\n",
    "    \"last_name_only\": 0.2, \n",
    "    \"transform_duplicates_end_character\" : 0.05\n",
    "}\n",
    "\n",
    "# Create training examples for all samples.\n",
    "all_examples = []\n",
    "for _, row in df_es.iterrows():\n",
    "    all_examples.extend(create_training_examples(row, transform_probs))\n",
    "\n",
    "# Print results.\n",
    "for example in all_examples[:10]:\n",
    "    print(\"Transformation:\", example[\"transformation\"])\n",
    "    print(\"Input:\", example[\"input\"])\n",
    "    print(\"Output:\", example[\"output\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575edc86-d7b7-47cb-af68-4f8bcf654f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29204113"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7d2260-cf7e-48fa-bc19-d2f4916d230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_es = pd.DataFrame(all_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ab81f6c1-f073-4efe-bfe6-72297642c7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>transformation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283789</th>\n",
       "      <td>Raza Ali</td>\n",
       "      <td>{\"first_name\": \"Raza\", \"last_name\": \"Ali\", \"ge...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11554589</th>\n",
       "      <td>carlos alvarez cabo</td>\n",
       "      <td>{\"first_name\": \"Carlos\", \"last_name\": \"Álvarez...</td>\n",
       "      <td>remove_accents_and_lower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17228826</th>\n",
       "      <td>AnDreA COrraLes Bujan</td>\n",
       "      <td>{\"first_name\": \"Andrea\", \"last_name\": \"Corrale...</td>\n",
       "      <td>random_uppercase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846174</th>\n",
       "      <td>mohamad zahir</td>\n",
       "      <td>{\"first_name\": \"Mohamad\", \"last_name\": \"Zahir\"...</td>\n",
       "      <td>remove_accents_and_lower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8752156</th>\n",
       "      <td>Manuel Palenzuela Gonzalez</td>\n",
       "      <td>{\"first_name\": \"Manuel\", \"last_name\": \"Palenzu...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22835797</th>\n",
       "      <td>Teresa</td>\n",
       "      <td>{\"first_name\": \"Teresa\", \"last_name\": null, \"g...</td>\n",
       "      <td>first_name_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15540819</th>\n",
       "      <td>Francisco   Segura</td>\n",
       "      <td>{\"first_name\": \"Francisco\", \"last_name\": \"Segu...</td>\n",
       "      <td>insert_extra_spaces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916549</th>\n",
       "      <td>Marisol Hornillos Lopez</td>\n",
       "      <td>{\"first_name\": \"Marisol\", \"last_name\": \"Hornil...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17152947</th>\n",
       "      <td>Sara Soriano</td>\n",
       "      <td>{\"first_name\": \"Sara\", \"last_name\": \"Soriano\",...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25248470</th>\n",
       "      <td>Ángel Álvarez</td>\n",
       "      <td>{\"first_name\": \"Ángel\", \"last_name\": \"Álvarez\"...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8852540</th>\n",
       "      <td>Álvaro Reche Navajas</td>\n",
       "      <td>{\"first_name\": \"Álvaro\", \"last_name\": \"Reche N...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455259</th>\n",
       "      <td>beatriz olalla dominguez</td>\n",
       "      <td>{\"first_name\": \"Beatriz\", \"last_name\": \"Olalla...</td>\n",
       "      <td>remove_accents_and_lower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416872</th>\n",
       "      <td>JuliánRomeraloFaubell</td>\n",
       "      <td>{\"first_name\": \"Julián\", \"last_name\": \"Romeral...</td>\n",
       "      <td>no_space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315220</th>\n",
       "      <td>Felipe</td>\n",
       "      <td>{\"first_name\": \"Felipe\", \"last_name\": null, \"g...</td>\n",
       "      <td>first_name_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433668</th>\n",
       "      <td>Montse  Álvarez</td>\n",
       "      <td>{\"first_name\": \"Montse\", \"last_name\": \"Álvarez...</td>\n",
       "      <td>insert_extra_spaces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778241</th>\n",
       "      <td>jose m gr</td>\n",
       "      <td>{\"first_name\": \"Jose M\", \"last_name\": \"Gr\", \"g...</td>\n",
       "      <td>remove_accents_and_lower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13480503</th>\n",
       "      <td>MiguelLaraPecino</td>\n",
       "      <td>{\"first_name\": \"Miguel\", \"last_name\": \"Lara Pe...</td>\n",
       "      <td>no_space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6894119</th>\n",
       "      <td>Mercedes</td>\n",
       "      <td>{\"first_name\": \"Mercedes\", \"last_name\": null, ...</td>\n",
       "      <td>first_name_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519121</th>\n",
       "      <td>Sergi   Gras   Jimenez</td>\n",
       "      <td>{\"first_name\": \"Sergi\", \"last_name\": \"Gras Jim...</td>\n",
       "      <td>insert_extra_spaces</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               input  \\\n",
       "8283789                     Raza Ali   \n",
       "11554589         carlos alvarez cabo   \n",
       "17228826       AnDreA COrraLes Bujan   \n",
       "11846174               mohamad zahir   \n",
       "8752156   Manuel Palenzuela Gonzalez   \n",
       "22835797                      Teresa   \n",
       "15540819          Francisco   Segura   \n",
       "4916549      Marisol Hornillos Lopez   \n",
       "17152947                Sara Soriano   \n",
       "25248470               Ángel Álvarez   \n",
       "8852540         Álvaro Reche Navajas   \n",
       "5455259     beatriz olalla dominguez   \n",
       "3416872        JuliánRomeraloFaubell   \n",
       "315220                        Felipe   \n",
       "1433668              Montse  Álvarez   \n",
       "3778241                    jose m gr   \n",
       "13480503            MiguelLaraPecino   \n",
       "6894119                     Mercedes   \n",
       "22519121      Sergi   Gras   Jimenez   \n",
       "\n",
       "                                                     output  \\\n",
       "8283789   {\"first_name\": \"Raza\", \"last_name\": \"Ali\", \"ge...   \n",
       "11554589  {\"first_name\": \"Carlos\", \"last_name\": \"Álvarez...   \n",
       "17228826  {\"first_name\": \"Andrea\", \"last_name\": \"Corrale...   \n",
       "11846174  {\"first_name\": \"Mohamad\", \"last_name\": \"Zahir\"...   \n",
       "8752156   {\"first_name\": \"Manuel\", \"last_name\": \"Palenzu...   \n",
       "22835797  {\"first_name\": \"Teresa\", \"last_name\": null, \"g...   \n",
       "15540819  {\"first_name\": \"Francisco\", \"last_name\": \"Segu...   \n",
       "4916549   {\"first_name\": \"Marisol\", \"last_name\": \"Hornil...   \n",
       "17152947  {\"first_name\": \"Sara\", \"last_name\": \"Soriano\",...   \n",
       "25248470  {\"first_name\": \"Ángel\", \"last_name\": \"Álvarez\"...   \n",
       "8852540   {\"first_name\": \"Álvaro\", \"last_name\": \"Reche N...   \n",
       "5455259   {\"first_name\": \"Beatriz\", \"last_name\": \"Olalla...   \n",
       "3416872   {\"first_name\": \"Julián\", \"last_name\": \"Romeral...   \n",
       "315220    {\"first_name\": \"Felipe\", \"last_name\": null, \"g...   \n",
       "1433668   {\"first_name\": \"Montse\", \"last_name\": \"Álvarez...   \n",
       "3778241   {\"first_name\": \"Jose M\", \"last_name\": \"Gr\", \"g...   \n",
       "13480503  {\"first_name\": \"Miguel\", \"last_name\": \"Lara Pe...   \n",
       "6894119   {\"first_name\": \"Mercedes\", \"last_name\": null, ...   \n",
       "22519121  {\"first_name\": \"Sergi\", \"last_name\": \"Gras Jim...   \n",
       "\n",
       "                    transformation  \n",
       "8283789                    correct  \n",
       "11554589  remove_accents_and_lower  \n",
       "17228826          random_uppercase  \n",
       "11846174  remove_accents_and_lower  \n",
       "8752156                    correct  \n",
       "22835797           first_name_only  \n",
       "15540819       insert_extra_spaces  \n",
       "4916549                    correct  \n",
       "17152947                   correct  \n",
       "25248470                   correct  \n",
       "8852540                    correct  \n",
       "5455259   remove_accents_and_lower  \n",
       "3416872                   no_space  \n",
       "315220             first_name_only  \n",
       "1433668        insert_extra_spaces  \n",
       "3778241   remove_accents_and_lower  \n",
       "13480503                  no_space  \n",
       "6894119            first_name_only  \n",
       "22519121       insert_extra_spaces  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df_es.sample(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "43554193-005b-4da1-8712-b95dc60c30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_es.to_parquet(\"df_es_clean_augmented.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1c8b7-9084-43ef-8099-4b0aafcd8110",
   "metadata": {},
   "source": [
    "#### Convert to {first}|{last}|{gender} format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b4b0523-e176-4ab9-93c3-a62e2c6b9901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "augmented_df_es = pd.read_parquet(\"df_es_clean_augmented.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467bc2cc-4b03-4570-b270-0f757d2ff233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(x):\n",
    "    data = json.loads(x)\n",
    "    # Replace None with an empty string\n",
    "    first = data.get(\"first_name\") or \"\"\n",
    "    last = data.get(\"last_name\") or \"\"\n",
    "    gender = data.get(\"gender\") or \"\"\n",
    "    return f\"{first}|{last}|{gender}\"\n",
    "\n",
    "df = augmented_df_es \n",
    "\n",
    "df['formatted_output'] = df['output'].apply(format_output)\n",
    "df[\"output\"] = df[\"formatted_output\"]\n",
    "\n",
    "df = df.drop(columns=[\"transformation\", \"formatted_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3097d04-f473-48dd-927e-04ca6b065a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26726287, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41066f6-0a19-41a9-a9a2-8ea1a3f266cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Split the dataframe (df has columns \"input\" and \"output\")\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d70fd41f-a770-4f13-9f58-114f6dd4e7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', '__index_level_0__'],\n",
       "        num_rows: 21648292\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'output', '__index_level_0__'],\n",
       "        num_rows: 2405366\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', '__index_level_0__'],\n",
       "        num_rows: 2672629\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad922ecd-ce85-48d2-92ca-4524a0ee3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "login(token = os.environ.get(\"HUGGINGFACE_AUTH_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0616e1-360b-4cc1-a500-61bbe26d08ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b937e312b13d43b6a07950898e6dcd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215a6031a92a408e969d2b025bcf5915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7217 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5690220855f3439e839727667762a47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7217 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c93650e3c14dd5a62c9e2471e4cda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7217 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f6aabeef904387b424c9acdef326ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f95e90bd6b4d81bfc1a5619817252e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2406 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fd34857dd5401689fe8c11d0d6297d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f43dc9284946478b087c253a0b2b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2673 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/juanluisrto/names-es/commit/d6d3a28dc1fd7ccc88d4072d4ef17dae97a823a4', commit_message='Upload dataset', commit_description='', oid='d6d3a28dc1fd7ccc88d4072d4ef17dae97a823a4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/juanluisrto/names-es', endpoint='https://huggingface.co', repo_type='dataset', repo_id='juanluisrto/names-es'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.push_to_hub(\"juanluisrto/names-es\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad-generation",
   "language": "python",
   "name": "ad-generation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
